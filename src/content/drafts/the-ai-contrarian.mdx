---
title: 'The AI contrarian'
description: ""
heroImage: "/blog/antropia/hero.png"
keywords: [ ]
pubDate: 'Feb 21 2026'
---

import SideNote from "../../components/blog/SideNote.astro"
import Callout from "../../components/blog/Callout.astro"

## Intro

I did a quick experiment to immerse myself into the AI hype, I ran a personal experiment and vibe-coded an app to aggregate your Meta ads and give you insightful of your running ads. I wrote my findings in [this post](https://antropia.studio/blog/to-ai-or-not-to-ai/), and **TL;DR: I didn't find vibe-coding very compelling**. With time, I have been able to refine my thoughts about it, this is what this post is all about.

I got to start off by saying I'm not anti-AI at all, and I use it almost daily, for multiple reasons: review my writings, ask for missing ideas in my explorations, or just as a powerful, context-aware StackOverflow search. If this sounds contradictory then we are getting places! This is exactly what I want to talk about, **the uneasiness I feel when using AI in today's world**.

The last reason I'm writing this post is to somehow give a voice to the many people I talk to regularly, colleagues and friends, who share a similar non-hyped view, but are too afraid to speak up. Some because their enthusiastic colleagues would drown their voice, others because their companies are **forcing** them (yeah, not suggesting, not encouraging, *forcing*) to use AI until they meet some arbitrary adoption metric. Hell, even I'm worried to post such a contrarian view (to what seems to have become mainstream now). As a part-freelancer myself I also work with clients who are placing high bets for AI. [to them I tell them not to worry (?). I don't know how to finish that sentence].

## Skill atrophy

While I was vibe-coding [adbrew](https://www.getadbrew.com/) I noticed something odd but also fascinating. The more I used AI, the more I relied on it. I wanted to test it all, with an open mind, I wanted to make sure if the experiment failed it wasn't because of my lack of trying: skills, MCPs, advanced prompting, planning, you name it. One day I noticed myself searching for the most basic TypeScript syntax error. I *knew* the answer, but my brain decided that it was easier to follow the path of least resistance: why think when I could just ask?

By week three I paused (and eventually stopped) the experiment, to reflect. Not because the tooling was particularly bad (even though it broke my way of working), but because **I was becoming worse**. I felt rusty. And I felt this way scarily fast.

It took me some time to recover. I spent days forcing myself not to ask the AI, recovering the lost confidence. And rewiring my muscle memory to prevent it from jumping into Claude for any minor inconvenience.

This has been widely reported so I'm not the only one who had the same experience[^1]. There is some articles already backing this up: the more people relied on AI tools, the less critical thinking they engaged in[^2].  I make mine the words of Luciano Nooijen[^3]:

> I started to feel a bit insecure about making some implementation decisions myself. Outsourcing the decisions to the AI seemed a lot easier. But sometimes, the AI couldn’t figure things out, even with the best prompts. It was quite clear that because I did not practice the basics often, I was less capable with the harder parts as well.

## The exhaustion

I've grown tired of reading AI-generated text. It's becoming painfully obvious when it is, the same cadence, the same filler transitions, the same confident emptiness. But that's media exhaustion, and everyone talks about it. There's another kind of exhaustion that I think is more interesting, and less discussed: **usage exhaustion**.

People who use AI tools heavily often describe feeling drained at the end of the day. The common interpretation is that they're exhausted from getting more done. I don't think that's it. I think working with agents forces you to context-switch so aggressively, jumping from prompting to reviewing to correcting to re-prompting, that it fragments the kind of deep focus that used to be the default mode of programming. You're not in flow. You're in a constant negotiation.

<SideNote note="Novelty is a powerful anaesthetic. It's why new tools always feel productive at first, regardless of whether they are. The energy comes from learning, not from output.">Right now, novelty covers for this</SideNote>. The tools are new, the possibilities feel endless, and there's a genuine thrill in watching something materialize from a prompt. But novelty fades. It always does. And when it does, what's left? If the satisfaction of solving a problem has been outsourced along with the implementation, we'll find ourselves in a familiar place: alienated from our own work, again. Except this time we'll have also created a hard dependency on the tool to remain employable.

Here's what I think is actually happening beneath the surface. Most of the time, making a good decision requires space. Time to grasp the problem, to properly evaluate trade-offs, to let ideas cross-reference themselves in the background while you're doing something else. Our previous workflows, for all their inefficiencies, gave us these gaps naturally. Writing code by hand is slow, yes, but that slowness is also thinking time. The friction is not a bug, it's where understanding lives.

When we delegate implementation to agents, we think we're being more productive because we're bringing more ideas to the prototype phase. But we're not doing more work. We're thinking less deeply about each idea and letting an LLM fill in the gap, spending significant computation and dollars to produce an 80% solution, when we could have spent that same time thinking hard and arriving at something we actually understand. The result is more prototypes, but shallower ones. More output, but less insight.

The METR study's finding, that developers felt 20% faster while actually being 19% slower[^9], points exactly in this direction. The perception gap isn't a statistical curiosity. It's a signal that something about the experience of using these tools is decoupled from reality. We feel productive because we're busy, because things are happening on screen, because the feedback loop is tight and stimulating. But busy is not the same as effective, and stimulating is not the same as satisfying.

These studies are still early, and I want to be honest about that. But I suspect this gap between perceived and actual productivity will only widen as the tools get smoother and the friction of using them decreases. The easier it becomes to skip the hard thinking, the more we'll skip it, and the less we'll notice we're doing so.

## Backpressure

Let's say, for the sake of argument, that AI genuinely makes code *generation* faster. Let's accept that premise completely, and also accept that generating more code is somehow a good metric. Does that solve the problem?

Coding speed has been one of software development's bottlenecks, sure. But it has never been the only one, and arguably not the most important. The others, understanding the problem, designing a solution, reviewing code, testing edge cases, debugging subtle interactions, maintaining the system over years, are inherently human. They're human because we're building software for humans: we aim for solving human problems with taste, reliability, delight [find other characteristics here for good software], and more. These aren't things you optimize with throughput.

What happens when you dramatically increase the volume of generated code? You've unblocked one bottleneck, congratulations. But the others remain, and they can't be easily LLM'ed away. More code means more code to review, more to test, more to understand, more to maintain. You've accelerated a part of the pipeline (that was already quite fast if you ask me), and created a pileup at every human checkpoint downstream.

```
┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│  [Code generation] ──▶ [Review] ──▶ [Testing] ──▶ [Maintenance] │
│                                                                 │
│  ████████████████       ███           ██            █            │
│  ▲ AI makes this       ▲ Human      ▲ Human       ▲ Human       │
│    explode               step         step          step         │
│                                                                 │
│  The bottleneck doesn't disappear. It moves.                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

The pattern here is **backpressure**. When one stage of a pipeline produces faster than the next stage can consume, pressure builds. And in software, that pressure has a name: it's a pull request waiting for review.

By making code generation generally available, we've also dramatically widened who can produce code. People with no coding background are now submitting pull requests, and to be clear, there is nothing wrong with that, more people building things is genuinely exciting. But each of those PRs still needs someone with experience to review it: to check for architectural coherence, security implications, edge cases, long-term maintainability. That review burden doesn't scale with the number of PRs generated. It scales with the number of experienced reviewers available, and that number hasn't changed.

<SideNote note="And before anyone says 'AI can review code too': sure, syntax-level checks and basic linting are getting close. But reviewing whether a design decision will hold up in six months, whether a data model makes sense for the business, whether a dependency is a liability, that's not pattern matching. That's judgment.">The natural response is to automate review too</SideNote>. And let's say we get there, partially. What happens next? The pressure doesn't vanish, it moves again. Now you have more code passing review, which means more edge cases to test, more integration points to validate, more security surfaces to audit. Automate testing? Now maintenance is overwhelmed. The ball keeps rolling downstream, and at every stop it finds a human who can't go any faster.

This is what backpressure looks like in practice: each bottleneck you "solve" with speed reveals the next one, and the next one is always harder to automate than the last, because it requires more context, more judgment, more taste. We're accelerating the easy parts and compressing the hard parts into less time with more volume. Under the image of speed, we're just redistributing pressure.


And the evidence suggests the code coming out the other end is not just more, it's more ephemeral. GitClear's analysis of 153 million lines of code found that "code churn", code rewritten within two weeks of being written, has roughly doubled since AI tools became widespread. Copy-pasted code is increasing faster than refactored, moved, or properly integrated code[^10]. An academic paper from December 2025 formalized this as the "flow-debt trade-off"[^11]: regeneration speed routinely outpaces human review, and vibe-coded MVPs that "look done" on the surface repeatedly failed basic security scans. The pattern in both studies points in the same direction: AI-generated code is cheaper to produce but more expensive to keep. Bugs compound, security issues hide in plain sight, architecture drifts without anyone noticing, and the dependency on the tool that created the mess deepens with every prompt. Tech debt doesn't just accumulate, it *accelerates*.

In my freelance work, I'm already seeing the consequences. I've had multiple conversations with potential clients who need their <SideNote note="Some of these are literally months old, built during the first wave of vibe-coding enthusiasm">recently built</SideNote> services rewritten because they've become unmaintainable. The irony is thick: the tool that promised to save development time is creating demand for experienced developers to clean up after it.

## The sigmoid delusion

Technology adoption tends to follow S-curves (sigmoid functions). Slow start, rapid growth, plateau. AI hype merchants want you to believe we're at the foot of the curve, that everything you're seeing is just the beginning of an exponential explosion.

But multiple lines of evidence suggest we may already be past the steep part.

A 2025 survey of 475 AI researchers found that 76% considered it "unlikely" or "very unlikely" that simply scaling up current LLMs would reach artificial general intelligence[^12]. This is a significant shift from the scaling-laws optimism of 2022-2023. GPT-5's release in mid-2025 exposed diminishing marginal returns on scaling, with performance gains largely leveling off across complex reasoning and commonsense tasks[^13]. Inside the labs, as HEC Paris put it, *"the consensus is growing that simply adding more data and compute will not create the all-knowing digital gods once promised."*[^14]

The people who build these models know the plateau is real. The people who *sell* them need you to believe it isn't.

I'm not saying there won't be progress. There almost certainly will be, new architectures, new approaches, new S-curves. But the specific curve we're on, the "scale it up and it gets smarter" curve, is flattening. And when you make trillion-dollar investment decisions on the assumption of indefinite exponential growth from a technology exhibiting logarithmic improvement, the math stops working.

People confuse the foot of a sigmoid with an exponential. They look the same at the beginning. They diverge very fast.

## Enshittification is not a risk, it's the playbook

Cory Doctorow's concept of enshittification, the predictable cycle where platforms start great, degrade to serve businesses, then degrade further to extract maximum shareholder value, is not a theoretical concern for AI products. **It's already happening.**

As of late 2025, Google has confirmed that ads are coming to AI Mode with no opt-out[^18]. OpenAI moved meaningful functionality behind a $200/month paywall, removing model choice from its $20 subscription and pushing all customization to its "pro" tier[^19]. Anthropic implemented aggressive rate limits on Claude Code right after closing a $5 billion funding round[^19]. Cursor, Copilot, every major AI tool, all launched premium tiers in 2025 that make the "free" experience meaningfully worse.

These are not the actions of companies confident in their product-market fit. These are the actions of companies burning cash faster than they can generate revenue ($40 billion in annual depreciation costs versus $15-20 billion in revenue for AI data centers in 2025[^20]), scrambling to extract value from users before the bill comes due.

If your entire workflow depends on a tool whose business model requires it to get worse for you over time, that's not a productivity gain. That's a trap. And we've seen this movie before with every other platform in the history of tech.

## Stop selling it

I have grown tired of reading AI-generated text (it's becoming painfully obvious when it is), but even more, I've grown tired of people *selling* AI. The hype doesn't feel genuine anymore. It feels rehearsed, performative, and increasingly desperate.

I'm not talking about the companies, their incentives are obvious. I'm talking about us. Engineers, developers, tech writers, conference speakers. Somewhere along the way, having a nuanced opinion became career-limiting. You either wave the AI flag enthusiastically or you're a Luddite who doesn't get it.

Here's what I'd ask: **talk about your experience, honestly**. Say what works, say what doesn't. Share what you've noticed. But don't sell what these companies are selling. You are not their marketing department. You don't get a commission. When you uncritically repeat the claim that *"AI will write 90% of code by next year"* you're not sharing knowledge, you're doing unpaid advertising for a company that scraped your Stack Overflow answers without asking.

The best articles I've read about AI this year were the honest ones. The ones that said *"I used it for X and it was great, I used it for Y and it was terrible, here's what I'm still figuring out."* That's useful. That helps people make decisions. The breathless "AI changed everything and if you're not using it you're dead" posts help nobody except the companies selling subscriptions.

## What I actually think

I don't want to end this as a pure critique, because that wouldn't be honest either. Here's where I actually land:

AI coding tools **are** useful for some things. Boilerplate, repetitive patterns, exploring unfamiliar APIs, generating test scaffolding. I use them. I'll continue to use them. They are part of the toolbox.

But they are not a revolution in how software is made. They are an incremental improvement to one specific part of a very long pipeline, wrapped in a marketing narrative that needs them to be much more than that to justify hundreds of billions of dollars in investment.

The things that make software valuable, understanding the problem deeply, designing systems that evolve gracefully, making trade-offs you can defend years later, working with humans who have contradictory needs, these things haven't gotten easier. If anything, they've gotten harder, because now there's more code to understand and more pressure to move fast.

So use the tools. But use them the way you'd use any tool: with intention, with awareness of their limits, and without letting them replace the skills that actually make you good at what you do. Code without AI sometimes. Not as a protest, but as practice, the way a musician still plays scales even when they have a synthesizer.

And be skeptical. Not cynical, skeptical. When someone tells you the future is certain and all you need to do is buy their product to participate in it, you've heard that pitch before. You know how it ends.

I don't have all the answers here. The technology is evolving, the evidence is accumulating, and my own views will probably shift as both of those continue. What I'm asking for is not rejection but *caution*. The kind of caution we'd apply to any other tool that promises to do our thinking for us.

If you've experienced something similar, or if you disagree entirely, I'd love to hear from you.

[^1]: [Avoiding Skill Atrophy in the Age of AI](https://addyo.substack.com/p/avoiding-skill-atrophy-in-the-age)
[^2]: [The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers](https://dl.acm.org/doi/full/10.1145/3706598.3713778)
[^3]: [Why I stopped using AI code editors](https://lucianonooijen.com/blog/why-i-stopped-using-ai-code-editors/)

[^1]: [AI coding is now everywhere. But not everyone is convinced — MIT Technology Review, December 2025](https://www.technologyreview.com/2025/12/15/1128352/rise-of-ai-coding-developers-2026/)
[^2]: [The Effects of Generative AI on Critical Thinking — Microsoft Research & Carnegie Mellon, 2025](https://www.microsoft.com/en-us/research/publication/the-effects-of-generative-ai-on-critical-thinking/)
[^3]: [Deskilling and upskilling with AI systems — Crowston & Bolici, Information Research Vol. 30, 2025](https://www.researchgate.net/publication/389742059_Deskilling_and_upskilling_with_AI_systems)
[^4]: [Avoiding Skill Atrophy in the Age of AI — Addy Osmani, April 2025](https://addyo.substack.com/p/avoiding-skill-atrophy-in-the-age)
[^5]: [GitHub Copilot research, Google internal studies, Microsoft Developer Division research (2023-2024)](https://github.blog/news-insights/research/)
[^6]: [2025 Stack Overflow Developer Survey — AI section](https://survey.stackoverflow.co/2025/ai)
[^7]: [AI Coding Assistant Statistics & Trends 2025 — Second Talent](https://www.secondtalent.com/resources/ai-coding-assistant-statistics/)
[^8]: [The AI Productivity Paradox Report 2025 — Faros AI](https://www.faros.ai/blog/ai-software-engineering)
[^9]: [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity — METR, July 2025](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)
[^10]: [AI in Software Development: Productivity at the Cost of Code Quality? — DevOps.com / GitClear, 2025](https://devops.com/ai-in-software-development-productivity-at-the-cost-of-code-quality-2/)
[^11]: [Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use — Waseem et al., arXiv, December 2025](https://arxiv.org/abs/2512.11922)
[^12]: [Survey of AI Scientists on Scaling and AGI, 2025 — cited in Generative AI and the Diminishing Returns of Scale](https://shaunyap01.github.io/articles/generative-ai-and-the-diminishing-returns-of-scale-are-we-nearing-the-plateau/)
[^13]: [AI Progress Plateau in 2025: Economic and Strategic Implications — AI 2 Work](https://ai2.work/news/ai-market-progress-plateau-and-economic-impact-2025/)
[^14]: [AI Beyond the Scaling Laws — HEC Paris](https://www.hec.edu/en/dare/tech-ai/ai-beyond-scaling-laws)
[^15]: [Copyright Office Report on AI Training and Fair Use — U.S. Copyright Office, May 2025 (analysis by McGuireWoods)](https://www.mcguirewoods.com/client-resources/alerts/2025/9/copyright-and-generative-ai-recent-developments-on-the-use-of-copyrighted-works-in-ai/)
[^16]: [AI Copyright Lawsuit Developments in 2025: A Year in Review — Copyright Alliance, January 2026](https://copyrightalliance.org/ai-copyright-lawsuit-developments-2025/)
[^17]: [Google Sues SerpApi Over Data Scraping — VKTR, December 2025 (on robots.txt and scraping practices)](https://www.vktr.com/ai-platforms/google-sues-serpapi-over-data-scraping/)
[^18]: [Google AI predicts its own enshittification timeline — Boing Boing, December 2025](https://boingboing.net/2025/12/19/google-ai-predicts-its-own-enshittification-timeline.html)
[^19]: [The Enshittification of Generative AI — Where's Your Ed At, September 2025](https://www.wheresyoured.at/the-enshittification-of-generative-ai/)
[^20]: [When Will AI Investments Start Paying Off? — GW&K Investment Management, 2025](https://www.gwkinvest.com/insight/macro/when-will-ai-investments-start-paying-off/)

---

# References to contrast ideas

- https://steve-yegge.medium.com/the-ai-vampire-eda6e4f07163
- https://mitchellh.com/writing/my-ai-adoption-journey
- https://addyo.substack.com/p/avoiding-skill-atrophy-in-the-age
- https://lucianonooijen.com/blog/why-i-stopped-using-ai-code-editors/
- https://www.reddit.com/r/ExperiencedDevs/comments/1raer4s/vibe_coders_passing_responsibility_to_code/
- https://www.reddit.com/r/ExperiencedDevs/comments/1r9iybj/the_gap_between_llm_functionality_and_social/ [i think it was deleted but has insightful comments anyways]
- https://www.thoughtworks.com/content/dam/thoughtworks/documents/report/tw_future%20_of_software_development_retreat_%20key_takeaways.pdf
- https://simonwillison.net/guides/agentic-engineering-patterns/code-is-cheap/
- https://news.ycombinator.com/item?id=47125374 comments

## Some ideas still missing

- Evidence is still foggy, with CEOs claiming the end of the world as we know it and independent studies claiming otherwise. How are pepole so bold and assertive when evidence is still not there?
- "*But Sergio, you are thinking about it all wrong, with agents you don't need to code anymore, so you don't need maintainable code, or at least not in the same way*". When that happens I'll re-evaluate, but that doesn't happen today, the AI still can't figure out
- But have you tried X model?
- Maybe include FAQ-like section as I did in https://antropia.studio/blog/on-risks-and-rewards/ with counter-arguments and my answer to them
- Add a section of CEOs claiming silly stuff that have not happened in their timeframe.

## Some ideas

- What are we losing by
  - Letting private companies claim the entire internet content with no permission to train models?
  - letting the LLM design and implement entire projects? We lose control
- The entire LLM ecosystem generates dependency.
  - Projects started with AI are hard to continue without it
  - After testing vide-coding for 3 weeks I noticed I found myself rusty and quick to jump into asking LLMs for the simplest of questions. It scared me and took me some time to recover control. (this can be the main point of the article)
- But have you tried X model?
- Stop selling what these companies are selling. Talk about it, your experience, but do not sell it.
- Evidence is still foggy, with CEOs claiming the end of the world as we know it and independent studies claiming otherwise. How are pepole so bold and assertive when evidence is still not there?
- What about the work it produces? More code is generated but we still have human steps in the funnel: review, testing, evaluation, exploration. [Diagram of a diamond growing and then finding a bottleneck in human steps]. Vibe coders are passing responsibility to code reviews.
- enshitification (adding ads, making models worse, or making models more expensive) is the book for these companies, and we already know it.
- People still believe sigmoid functions are exponential, and they only are in their foot.
- The freelance work has plenty of cases of people wanting to re-write their services so that they are maintainable.
- I've grown tired of reading AI-generated text (it's quite obvious when it is) and now I'm growing tired of people selling AI, the hype doesn't feel genuine.
- I wrote this article in my own name, but also in the names of those who I talk to almost daily, and have similar non-hyped points of view, but are too afraid to talk because their hyped colleagues will drown their voice, or because their companies are forcing (yes, forcing, not suggesting, nor encouraging) them to use AI until they meet some arbitrary metric.
- "*But Sergio, you are thinking about it all wrong, with agents you don't need to code anymore, so you don't need maintainable code, or at least not in the same way*". When that happens I'll re-evaluate, but that doesn't happen today, the AI still can't figure out
- What about juniors?
